{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Image Processing 1.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Khizer2/test-repo/blob/master/Image_Processing_1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "2Q0HFT-MRPpv",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import tensorflow as tf"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "6yl1Fe-wRb96",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from tensorflow import keras"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "9uEX3FM1RaNO",
        "colab_type": "code",
        "outputId": "a83b87a9-7935-492f-cd3d-49a262e56313",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "keras.preprocessing.image.ImageDataGenerator(featurewise_center=False, samplewise_center=False, featurewise_std_normalization=False, samplewise_std_normalization=False, zca_whitening=False, zca_epsilon=1e-06, rotation_range=0, width_shift_range=0.0, height_shift_range=0.0, brightness_range=None, shear_range=0.0, zoom_range=0.0, channel_shift_range=0.0, fill_mode='nearest', cval=0.0, horizontal_flip=False, vertical_flip=False, rescale=None, preprocessing_function=None, data_format=None, validation_split=0.0, dtype=None)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.preprocessing.image.ImageDataGenerator at 0x7f7cbf9d9b70>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "metadata": {
        "id": "xagGqRG9RiEh",
        "colab_type": "code",
        "outputId": "930eec30-4383-484d-fff4-9976d10c8f3d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 306
        }
      },
      "cell_type": "code",
      "source": [
        "# baseline cnn model for the cifar10 problem\n",
        "from keras.datasets.cifar10 import load_data\n",
        "from keras.utils import to_categorical\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Conv2D\n",
        "from keras.layers import MaxPooling2D\n",
        "from keras.layers import Dense\n",
        "from keras.layers import Flatten\n",
        "from keras.layers import BatchNormalization\n",
        "# load dataset\n",
        "(trainX, trainY), (testX, testY) = load_data()\n",
        "# normalize pixel values\n",
        "trainX = trainX.astype('float32') / 255\n",
        "testX = testX.astype('float32') / 255\n",
        "# one hot encode target values\n",
        "trainY = to_categorical(trainY)\n",
        "testY = to_categorical(testY)\n",
        "# define model\n",
        "model = Sequential()\n",
        "model.add(Conv2D(32, (3, 3), activation='relu', padding='same', kernel_initializer='he_uniform', input_shape=(32, 32, 3)))\n",
        "model.add(BatchNormalization())\n",
        "model.add(MaxPooling2D((2, 2)))\n",
        "model.add(Conv2D(64, (3, 3), activation='relu', padding='same', kernel_initializer='he_uniform'))\n",
        "model.add(BatchNormalization())\n",
        "model.add(MaxPooling2D((2, 2)))\n",
        "model.add(Flatten())\n",
        "model.add(Dense(128, activation='relu', kernel_initializer='he_uniform'))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Dense(10, activation='softmax'))\n",
        "# compile model\n",
        "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "# fit model\n",
        "history = model.fit(trainX, trainY, epochs=3, batch_size=128)\n",
        "# evaluate model\n",
        "_, acc = model.evaluate(testX, testY, verbose=0)\n",
        "print(acc)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz\n",
            "170500096/170498071 [==============================] - 10s 0us/step\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Colocations handled automatically by placer.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.cast instead.\n",
            "Epoch 1/3\n",
            "50000/50000 [==============================] - 242s 5ms/step - loss: 1.2409 - acc: 0.5651\n",
            "Epoch 2/3\n",
            "50000/50000 [==============================] - 245s 5ms/step - loss: 0.8559 - acc: 0.7007\n",
            "Epoch 3/3\n",
            "50000/50000 [==============================] - 258s 5ms/step - loss: 0.6754 - acc: 0.7640\n",
            "0.6487\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "Oj96LEShXrIY",
        "colab_type": "code",
        "outputId": "bb5e5ab0-42c5-41da-8179-86ef72b25237",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 202
        }
      },
      "cell_type": "code",
      "source": [
        "# baseline cnn model for the cifar10 problem, repeated evaluation\n",
        "from numpy import mean\n",
        "from numpy import std\n",
        "from keras.datasets.cifar10 import load_data\n",
        "from keras.utils import to_categorical\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Conv2D\n",
        "from keras.layers import MaxPooling2D\n",
        "from keras.layers import Dense\n",
        "from keras.layers import Flatten\n",
        "from keras.layers import BatchNormalization\n",
        "\n",
        "# load and return the cifar10 dataset ready for modeling\n",
        "def load_dataset():\n",
        "\t# load dataset\n",
        "\t(trainX, trainY), (testX, testY) = load_data()\n",
        "\t# normalize pixel values\n",
        "\ttrainX = trainX.astype('float32') / 255\n",
        "\ttestX = testX.astype('float32') / 255\n",
        "\t# one hot encode target values\n",
        "\ttrainY = to_categorical(trainY)\n",
        "\ttestY = to_categorical(testY)\n",
        "\treturn trainX, trainY, testX, testY\n",
        "\n",
        "# define the cnn model for the cifar10 dataset\n",
        "def define_model():\n",
        "\t# define model\n",
        "\tmodel = Sequential()\n",
        "\tmodel.add(Conv2D(32, (3, 3), activation='relu', padding='same', kernel_initializer='he_uniform', input_shape=(32, 32, 3)))\n",
        "\tmodel.add(BatchNormalization())\n",
        "\tmodel.add(MaxPooling2D((2, 2)))\n",
        "\tmodel.add(Conv2D(64, (3, 3), activation='relu', padding='same', kernel_initializer='he_uniform'))\n",
        "\tmodel.add(BatchNormalization())\n",
        "\tmodel.add(MaxPooling2D((2, 2)))\n",
        "\tmodel.add(Flatten())\n",
        "\tmodel.add(Dense(128, activation='relu', kernel_initializer='he_uniform'))\n",
        "\tmodel.add(BatchNormalization())\n",
        "\tmodel.add(Dense(10, activation='softmax'))\n",
        "\t# compile model\n",
        "\tmodel.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\treturn model\n",
        "\n",
        "# fit and evaluate a defined model\n",
        "def evaluate_model(model, trainX, trainY, testX, testY):\n",
        "\t# fit model\n",
        "\tmodel.fit(trainX, trainY, epochs=3, batch_size=128, verbose=0)\n",
        "\t# evaluate model\n",
        "\t_, acc = model.evaluate(testX, testY, verbose=0)\n",
        "\treturn acc\n",
        "\n",
        "# repeatedly evaluate model, return distribution of scores\n",
        "def repeated_evaluation(trainX, trainY, testX, testY, repeats=10):\n",
        "\tscores = list()\n",
        "\tfor _ in range(repeats):\n",
        "\t\t# define model\n",
        "\t\tmodel = define_model()\n",
        "\t\t# fit and evaluate model\n",
        "\t\taccuracy = evaluate_model(model, trainX, trainY, testX, testY)\n",
        "\t\t# store score\n",
        "\t\tscores.append(accuracy)\n",
        "\t\tprint('> %.3f' % accuracy)\n",
        "\treturn scores\n",
        "\n",
        "# load dataset\n",
        "trainX, trainY, testX, testY = load_dataset()\n",
        "# evaluate model\n",
        "scores = repeated_evaluation(trainX, trainY, testX, testY)\n",
        "# summarize result\n",
        "print('Accuracy: %.3f (%.3f)' % (mean(scores), std(scores)))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "> 0.669\n",
            "> 0.685\n",
            "> 0.693\n",
            "> 0.679\n",
            "> 0.695\n",
            "> 0.642\n",
            "> 0.683\n",
            "> 0.678\n",
            "> 0.680\n",
            "> 0.691\n",
            "Accuracy: 0.680 (0.014)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "7Ru3Joiy1uvw",
        "colab_type": "code",
        "outputId": "24b4b4c2-deb3-4435-eeda-5b6fa1e61194",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 151
        }
      },
      "cell_type": "code",
      "source": [
        "# cnn model for the cifar10 problem with test-time augmentation\n",
        "import numpy\n",
        "from numpy import argmax\n",
        "from numpy import mean\n",
        "from numpy import std\n",
        "from numpy import expand_dims\n",
        "from sklearn.metrics import accuracy_score\n",
        "from keras.datasets.cifar10 import load_data\n",
        "from keras.utils import to_categorical\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Conv2D\n",
        "from keras.layers import MaxPooling2D\n",
        "from keras.layers import Dense\n",
        "from keras.layers import Flatten\n",
        "from keras.layers import BatchNormalization\n",
        "\n",
        "# load and return the cifar10 dataset ready for modeling\n",
        "def load_dataset():\n",
        "\t# load dataset\n",
        "\t(trainX, trainY), (testX, testY) = load_data()\n",
        "\t# normalize pixel values\n",
        "\ttrainX = trainX.astype('float32') / 255\n",
        "\ttestX = testX.astype('float32') / 255\n",
        "\t# one hot encode target values\n",
        "\ttrainY = to_categorical(trainY)\n",
        "\ttestY = to_categorical(testY)\n",
        "\treturn trainX, trainY, testX, testY\n",
        "\n",
        "# define the cnn model for the cifar10 dataset\n",
        "def define_model():\n",
        "\t# define model\n",
        "\tmodel = Sequential()\n",
        "\tmodel.add(Conv2D(32, (3, 3), activation='relu', padding='same', kernel_initializer='he_uniform', input_shape=(32, 32, 3)))\n",
        "\tmodel.add(BatchNormalization())\n",
        "\tmodel.add(MaxPooling2D((2, 2)))\n",
        "\tmodel.add(Conv2D(64, (3, 3), activation='relu', padding='same', kernel_initializer='he_uniform'))\n",
        "\tmodel.add(BatchNormalization())\n",
        "\tmodel.add(MaxPooling2D((2, 2)))\n",
        "\tmodel.add(Flatten())\n",
        "\tmodel.add(Dense(128, activation='relu', kernel_initializer='he_uniform'))\n",
        "\tmodel.add(BatchNormalization())\n",
        "\tmodel.add(Dense(10, activation='softmax'))\n",
        "\t# compile model\n",
        "\tmodel.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\treturn model\n",
        "\n",
        "# make a prediction using test-time augmentation\n",
        "def tta_prediction(datagen, model, image, n_examples):\n",
        "\t# convert image into dataset\n",
        "\tsamples = expand_dims(image, 0)\n",
        "\t# prepare iterator\n",
        "\tit = datagen.flow(samples, batch_size=n_examples)\n",
        "\t# make predictions for each augmented image\n",
        "\tyhats = model.predict_generator(it, steps=n_examples, verbose=0)\n",
        "\t# sum across predictions\n",
        "\tsummed = numpy.sum(yhats, axis=0)\n",
        "\t# argmax across classes\n",
        "\treturn argmax(summed)\n",
        "\n",
        "# evaluate a model on a dataset using test-time augmentation\n",
        "def tta_evaluate_model(model, testX, testY):\n",
        "\t# configure image data augmentation\n",
        "\tdatagen = ImageDataGenerator(horizontal_flip=True)\n",
        "\t# define the number of augmented images to generate per test set image\n",
        "\tn_examples_per_image = 7\n",
        "\tyhats = list()\n",
        "\tfor i in range(len(testX)):\n",
        "\t\t# make augmented prediction\n",
        "\t\tyhat = tta_prediction(datagen, model, testX[i], n_examples_per_image)\n",
        "\t\t# store for evaluation\n",
        "\t\tyhats.append(yhat)\n",
        "\t# calculate accuracy\n",
        "\ttestY_labels = argmax(testY, axis=1)\n",
        "\tacc = accuracy_score(testY_labels, yhats)\n",
        "\treturn acc\n",
        "\n",
        "# fit and evaluate a defined model\n",
        "def evaluate_model(model, trainX, trainY, testX, testY):\n",
        "\t# fit model\n",
        "\tmodel.fit(trainX, trainY, epochs=3, batch_size=128, verbose=0)\n",
        "\t# evaluate model using tta\n",
        "\tacc = tta_evaluate_model(model, testX, testY)\n",
        "\treturn acc\n",
        "\n",
        "# repeatedly evaluate model, return distribution of scores\n",
        "def repeated_evaluation(trainX, trainY, testX, testY, repeats=10):\n",
        "\tscores = list()\n",
        "\tfor _ in range(repeats):\n",
        "\t\t# define model\n",
        "\t\tmodel = define_model()\n",
        "\t\t# fit and evaluate model\n",
        "\t\taccuracy = evaluate_model(model, trainX, trainY, testX, testY)\n",
        "\t\t# store score\n",
        "\t\tscores.append(accuracy)\n",
        "\t\tprint('> %.3f' % accuracy)\n",
        "\treturn scores\n",
        "\n",
        "# load dataset\n",
        "trainX, trainY, testX, testY = load_dataset()\n",
        "# evaluate model\n",
        "scores = repeated_evaluation(trainX, trainY, testX, testY)\n",
        "# summarize result\n",
        "print('Accuracy: %.3f (%.3f)' % (mean(scores), std(scores)))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "> 0.691\n",
            "> 0.631\n",
            "> 0.675\n",
            "> 0.687\n",
            "> 0.694\n",
            "> 0.688\n",
            "> 0.699\n",
            "> 0.672\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}